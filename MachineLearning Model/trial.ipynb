{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from rembg import remove\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This script extracts dominant colors from an image and recommends perfumes based on:\n",
    "- **Color detection** â†’ Matches colors with olfactive accords using improved histogram-based detection.\n",
    "- **Face removal** â†’ Detects and removes faces before color extraction.\n",
    "- **Situation-based filtering** â†’ Suggests fragrances based on context (e.g., \"formal event\").\n",
    "- **Preference filtering** â†’ Recommends perfumes containing a specific accord.\n",
    "- **Exclusion filtering** â†’ Avoids perfumes with unwanted accords.\n",
    "- **Gender filtering** â†’ Returns perfumes based on gender (`Male`, `Female`, `Unisex`).\n",
    "- **Text-based recommendation (TF-IDF + KNN)** â†’ Finds similar perfumes based on olfactive descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loading fragrance dataset...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Step 1: Load the Fragrance Database\n",
    "display(\"Loading fragrance dataset...\")\n",
    "fragrance_df = pd.read_csv(\"../data/fragrance_ML_model.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Text-Based Fragrance Recommendation System (TF-IDF + KNN)\n",
    "display(\"Setting up text-based recommendation system...\")\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "feature_vectors = vectorizer.fit_transform(fragrance_df[\"Olfactive Profile\"])\n",
    "knn_model = NearestNeighbors(n_neighbors=20, metric=\"cosine\", algorithm=\"brute\")\n",
    "knn_model.fit(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_text(query):\n",
    "    display(f\"Searching for perfumes similar to: {query}\")\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    distances, indices = knn_model.kneighbors(query_vector)\n",
    "    recommended = fragrance_df.iloc[indices[0]]\n",
    "    return recommended.sample(frac=1).reset_index(drop=True)  # Shuffle recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Remove Background and Faces from Image\n",
    "def remove_background(image_path):\n",
    "    display(\"Removing background and detecting faces in image...\")\n",
    "    image = Image.open(image_path)\n",
    "    output = remove(image)\n",
    "    processed_image_path = \"processed_image.png\"\n",
    "    output.save(processed_image_path)\n",
    "\n",
    "\n",
    "    # Load the processed image with OpenCV\n",
    "    image_cv = cv2.imread(processed_image_path)\n",
    "    gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # Replace detected faces with a neutral color (gray)\n",
    "    for (x, y, w, h) in faces:\n",
    "        image_cv[y:y+h, x:x+w] = (128, 128, 128)  # Gray out the face\n",
    "    \n",
    "    processed_face_path = \"processed_face_image.png\"\n",
    "    cv2.imwrite(processed_face_path, image_cv)\n",
    "    return processed_face_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from rembg import remove\n",
    "\n",
    "def remove_non_clothing(image_path):\n",
    "    \"\"\"\n",
    "    - Elimina el fondo de la imagen (rembg)\n",
    "    - Detecta y elimina la piel (cara, manos)\n",
    "    - Mantiene solo la ropa\n",
    "    \n",
    "    Retorna:\n",
    "    - processed_clothing_path (str): Ruta de la imagen final con solo la ropa.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ‘• Eliminando fondo y manteniendo solo la ropa en la imagen...\")\n",
    "\n",
    "    # **PASO 1: Eliminar fondo con rembg**\n",
    "    image = Image.open(image_path)\n",
    "    output = remove(image)\n",
    "    processed_image_path = \"processed_no_bg.png\"\n",
    "    output.save(processed_image_path)\n",
    "\n",
    "    # **PASO 2: Cargar imagen y convertir a HSV**\n",
    "    image_cv = cv2.imread(processed_image_path)\n",
    "    hsv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # **PASO 3: Detectar piel y eliminarla**\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)  # Tono piel bajo\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)  # Tono piel alto\n",
    "    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Aplicar la mÃ¡scara para eliminar Ã¡reas de piel\n",
    "    image_no_skin = cv2.bitwise_and(image_cv, image_cv, mask=cv2.bitwise_not(skin_mask))\n",
    "\n",
    "    # **PASO 4: Filtrar objetos pequeÃ±os (mantener solo ropa)**\n",
    "    gray = cv2.cvtColor(image_no_skin, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area < 5000:  # Eliminar objetos pequeÃ±os (caras, manos)\n",
    "            cv2.drawContours(image_no_skin, [cnt], -1, (0, 0, 0), thickness=cv2.FILLED)  # Negro\n",
    "\n",
    "    # **PASO 5: Guardar la imagen final**\n",
    "    processed_clothing_path = \"processed_clothing_only.png\"\n",
    "    cv2.imwrite(processed_clothing_path, image_no_skin)\n",
    "\n",
    "    print(f\"âœ… Imagen procesada guardada en: {processed_clothing_path}\")\n",
    "    return processed_clothing_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘• Eliminando fondo y manteniendo solo la ropa en la imagen...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\celia\\\\Documents\\\\IronhackDataAnalysis\\\\FinalProject\\\\MachineLearning Model\\\\test_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Imagen original\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m processed_clothing_path \u001b[38;5;241m=\u001b[39m \u001b[43mremove_non_clothing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Imagen final guardada en: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_clothing_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mremove_non_clothing\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ‘• Eliminando fondo y manteniendo solo la ropa en la imagen...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# **PASO 1: Eliminar fondo con rembg**\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m remove(image)\n\u001b[0;32m     20\u001b[0m processed_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_no_bg.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\celia\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\celia\\\\Documents\\\\IronhackDataAnalysis\\\\FinalProject\\\\MachineLearning Model\\\\test_image.jpg'"
     ]
    }
   ],
   "source": [
    "image_path = \"test_image.jpg\"  # Imagen original\n",
    "processed_clothing_path = remove_non_clothing(image_path)\n",
    "\n",
    "print(f\"âœ… Imagen final guardada en: {processed_clothing_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Extract Dominant Colors using Histogram Analysis\n",
    "def extract_dominant_colors(image_path, num_colors=3):\n",
    "    display(\"Extracting dominant colors using histogram analysis...\")\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(\"Image not found in this path\")\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    hist = cv2.calcHist([image], [0], None, [180], [0, 180])\n",
    "    hist = hist.flatten()\n",
    "\n",
    "    hist_percentage = (hist/ hist.sum()) * 100\n",
    "    sorted_indices = np.argsort(hist_percentage)[::-1]  #Order by most frequent colors\n",
    "\n",
    "    top_colors = [hue for hue in sorted_indices if hist_percentage[hue] >= min_percentage][:num_colors]\n",
    "    \n",
    "    top_colors = np.argsort(hist)[-num_colors:][::-1]  # Get top N most frequent colors\n",
    "    \n",
    "    color_labels = []\n",
    "    for hue in top_colors:\n",
    "        if 0 <= hue < 10 or hue >= 170:\n",
    "            color_labels.append(\"red\")\n",
    "        elif 10 <= hue < 25:\n",
    "            color_labels.append(\"orange\")\n",
    "        elif 25 <= hue < 35:\n",
    "            color_labels.append(\"yellow\")\n",
    "        elif 35 <= hue < 85:\n",
    "            color_labels.append(\"green\")\n",
    "        elif 85 <= hue < 130:\n",
    "            color_labels.append(\"blue\")\n",
    "        elif 130 <= hue < 160:\n",
    "            color_labels.append(\"purple\")\n",
    "        elif 160 <= hue < 170:\n",
    "            color_labels.append(\"pink\")\n",
    "        elif 170 <= hue < 195:\n",
    "            color_labels.append(\"black\")\n",
    "        elif 195 <= hue < 225:\n",
    "            color_labels.append(\"white\")\n",
    "        elif 225 <= hue < 245:\n",
    "            color_labels.append(\"beige\")\n",
    "        elif 245 <= hue < 265:\n",
    "            color_labels.append(\"brown\")\n",
    "    \n",
    "    return list(set(color_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Map Colors to Olfactive Accords\n",
    "COLOR_TO_ACCORD = {\n",
    "    \"yellow\": [\"citrus\", \"fruity\", \"sweet\"],\n",
    "    \"blue\": [\"marine\", \"aquatic\", \"ozonic\"],\n",
    "    \"beige\": [\"powdery\", \"soft spicy\", \"musky\"],\n",
    "    \"white\": [\"clean\", \"aldehyde\", \"soft spicy\"],\n",
    "    \"red\": [\"spicy\", \"woody\", \"amber\"],\n",
    "    \"green\": [\"herbal\", \"fresh spicy\", \"aromatic\"],\n",
    "    \"black\": [\"dark\", \"intense\", \"leathery\"],\n",
    "    \"pink\": [\"floral\", \"sweet\", \"fruity\"],\n",
    "    \"orange\": [\"warm spicy\", \"sweet\", \"gourmand\"],\n",
    "    \"purple\": [\"powdery\", \"floral\", \"woody\"],\n",
    "    \"brown\": [\"woody\", \"earthy\", \"leathery\"]\n",
    "}\n",
    "\n",
    "def map_colors_to_accords(dominant_colors):\n",
    "    detected_accords = []\n",
    "    for color in dominant_colors:\n",
    "        if color in COLOR_TO_ACCORD:\n",
    "            accords.extend(COLOR_TO_ACCORD[color])\n",
    "    return list(set(accords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Situation-based accords\n",
    "SITUATION_TO_ACCORD = {\n",
    "    \"formal\": [\"woody\", \"spicy\", \"leather\"],\n",
    "    \"casual\": [\"fresh\", \"citrus\", \"aquatic\"],\n",
    "    \"romantic\": [\"floral\", \"sweet\", \"musky\"],\n",
    "    \"sport\": [\"green\", \"aquatic\", \"ozonic\"],\n",
    "    \"office\": [\"clean\", \"powdery\", \"aldehyde\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Define the Recommendation Function\n",
    "def recommend_fragrance(input_data, mode=\"image\", situation=None, preference=None, exclude=None, gender=None, num_recommendations=5):\n",
    "    display(\"Generating fragrance recommendation...\")\n",
    "    if mode == \"text\":\n",
    "        return recommend_by_text(input_data)\n",
    "    \n",
    "    processed_image_path = remove_background(input_data)\n",
    "    detected_colors = extract_dominant_colors(processed_image_path)\n",
    "    accords_segmented = [COLOR_TO_ACCORD[color] for color in detected_colors if color in COLOR_TO_ACCORD]\n",
    "    \n",
    "    combined_accords = list(set(sum(accords_segmented, [])))\n",
    "    if situation and situation in SITUATION_TO_ACCORD:\n",
    "        combined_accords.extend(SITUATION_TO_ACCORD[situation])\n",
    "    combined_accords = list(set(combined_accords))\n",
    "    \n",
    "    filtered_df = fragrance_df[fragrance_df[\"Olfactive Profile\"].str.contains('|'.join(combined_accords), case=False, na=False)]\n",
    "    \n",
    "    if gender:\n",
    "        filtered_df = filtered_df[filtered_df[\"Gender\"].str.contains(gender, case=False, na=False)]\n",
    "    if preference:\n",
    "        filtered_df = filtered_df[filtered_df[\"Olfactive Profile\"].str.contains(preference, case=False, na=False)]\n",
    "    if exclude:\n",
    "        filtered_df = filtered_df[~filtered_df[\"Olfactive Profile\"].str.contains(exclude, case=False, na=False)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        return \"No perfumes found after applying filters. Try adjusting your criteria.\"\n",
    "    \n",
    "    return filtered_df.sample(n=min(num_recommendations, len(filtered_df))).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Run Sample Tests with Detailed Analysis\n",
    "\n",
    "def run_sample_tests():\n",
    "    # Image-based Recommendation Test\n",
    "    display(\"Running detailed sample test for image-based recommendation...\")\n",
    "    test_image_path = \"test_image.png\"\n",
    "    \n",
    "    display(f\"Using test image: {test_image_path}\")\n",
    "    processed_image = remove_background(test_image_path)\n",
    "    \n",
    "    # Display original and processed images\n",
    "    original = Image.open(test_image_path)\n",
    "    processed = Image.open(processed_image)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(processed)\n",
    "    plt.title(\"Processed Image (Face Removed)\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    detected_colors = extract_dominant_colors(processed_image)\n",
    "    display(f\"Detected Colors: {detected_colors}\")\n",
    "    \n",
    "    recommendations = recommend_fragrance(\n",
    "        input_data=test_image_path,\n",
    "        mode=\"image\",\n",
    "        situation=\"casual\",\n",
    "        preference=\"lemon\",\n",
    "        exclude=\"sweet\",\n",
    "        gender=\"unisex\",\n",
    "        num_recommendations=5\n",
    "    )\n",
    "    display(\"Recommended Fragrances:\", recommendations)\n",
    "\n",
    "     # Text-based Recommendation Test\n",
    "    display(\"Running detailed sample test for text-based recommendation...\")\n",
    "    text_query = \"fresh citrus\"\n",
    "    display(f\"Input description: {text_query}\")\n",
    "    text_recommendations = recommend_by_text(text_query)\n",
    "    display(\"Recommended Fragrances:\", text_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute sample tests\n",
    "run_sample_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Upload a test image interactively\n",
    "uploader = widgets.FileUpload(accept='.png,.jpg,.jpeg', multiple=False)\n",
    "display(uploader)\n",
    "\n",
    "if uploader.value:\n",
    "    uploaded_file = list(uploader.value.values())[0]\n",
    "    test_image_path = \"test_image.png\"\n",
    "    with open(test_image_path, \"wb\") as f:\n",
    "        f.write(uploaded_file['content'])\n",
    "    display(f\"Image saved as {test_image_path}\")\n",
    "else:\n",
    "    display(\"No file uploaded! Please upload an image first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
