{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrembg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32mc:\\Users\\celia\\anaconda3\\Lib\\site-packages\\rembg\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _version\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m _version\u001b[38;5;241m.\u001b[39mget_versions()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m new_session\n",
      "File \u001b[1;32mc:\\Users\\celia\\anaconda3\\Lib\\site-packages\\rembg\\bg.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Tuple, Union, cast\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mort\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     BORDER_DEFAULT,\n\u001b[0;32m      9\u001b[0m     MORPH_ELLIPSE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     morphologyEx,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "import numpy as np\n",
    "from rembg import remove\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fragrance Recommendation System\n",
    "\n",
    "## Overview\n",
    "This script extracts dominant colors from an image and recommends perfumes based on:\n",
    "- **Color detection** → Matches colors with olfactive accords.\n",
    "- **Situation-based filtering** → Suggests fragrances based on context (e.g., \"formal event\").\n",
    "- **Preference filtering** → Recommends perfumes containing a specific accord.\n",
    "- **Exclusion filtering** → Avoids perfumes with unwanted accords.\n",
    "- **Gender filtering** → Returns perfumes based on gender (`Male`, `Female`, `Unisex`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Fragrance Database\n",
    "This loads the dataset containing perfume information, including olfactive profiles and gender classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the database created for the fragrance recomendation system\n",
    "fragrance_df = pd.read_csv(\"../data/fragrance_ML_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define fuction to remove the background from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_background(input_path, output_path):\n",
    "    \"\"\"Removes background from an image using rembg.\"\"\"\n",
    "    image = Image.open(input_path)\n",
    "    output = remove(image)\n",
    "    output.save(output_path)\n",
    "    print(f\" Background removed successfully! Saved as {output_path}\")\n",
    "    output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Dominant Colors from uploaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dominant_colors(image_path, num_colors=3):\n",
    "    \"\"\"\n",
    "    Extracts the most dominant colors from an image.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((100, 100))  # Reduce size for performance\n",
    "    \n",
    "    image_np = np.array(image)\n",
    "    pixels = image_np.reshape(-1, 3)  # Convert to a list of RGB pixels\n",
    "    \n",
    "    color_counts = Counter(map(tuple, pixels))\n",
    "    dominant_colors = [color for color, _ in color_counts.most_common(num_colors)]\n",
    "    \n",
    "    return dominant_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Map Colors to Olfactive Accords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_TO_ACCORD = {\n",
    "    (255, 255, 0): [\"citrus\", \"fruity\", \"sweet\"],  # Yellow\n",
    "    (0, 0, 255): [\"marine\", \"aquatic\", \"ozonic\"],  # Blue\n",
    "    (245, 245, 220): [\"powdery\", \"soft spicy\", \"musky\"],  # Beige\n",
    "    (255, 255, 255): [\"clean\", \"aldehyde\", \"soft spicy\"],  # White\n",
    "    (128, 128, 128): [\"musky\", \"powdery\", \"earthy\"],  # Gray\n",
    "    (255, 0, 255): [\"sweet\", \"fruity\", \"floral\"],  # Magenta\n",
    "    (139, 69, 19): [\"woody\", \"amber\", \"leather\"],  # Brown\n",
    "    (210, 180, 140): [\"warm spicy\", \"woody\", \"sweet\"],  # Light Brown\n",
    "    (192, 192, 192): [\"aldehyde\", \"fresh\", \"clean\"],  # Metallic\n",
    "    (255, 165, 0): [\"citrus\", \"spicy\", \"warm\"],  # Orange\n",
    "    (0, 0, 0): [\"leather\", \"smoky\", \"dark\"],  # Black\n",
    "    (255, 0, 0): [\"spicy\", \"warm\", \"oriental\"],  # Red\n",
    "    (255, 192, 203): [\"floral\", \"powdery\", \"sweet\"],  # Pink\n",
    "    (0, 255, 0): [\"fresh\", \"herbal\", \"green\"]  # Green\n",
    "}\n",
    "\n",
    "def extract_dominant_colors(image_path, num_colors=3):\n",
    "    \"\"\"\n",
    "    Extracts the most dominant colors from an image using K-means clustering.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((100, 100))\n",
    "    \n",
    "    image_np = np.array(image)\n",
    "    pixels = image_np.reshape(-1, 3)\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=num_colors, random_state=0)\n",
    "    kmeans.fit(pixels)\n",
    "    dominant_colors = kmeans.cluster_centers_.astype(int)\n",
    "    \n",
    "    return [tuple(color) for color in dominant_colors]\n",
    "\n",
    "def map_colors_to_accords(dominant_colors):\n",
    "    \"\"\"Maps extracted colors to their corresponding olfactive accords.\"\"\"\n",
    "    accords = []\n",
    "    for color in dominant_colors:\n",
    "        closest_color = min(COLOR_TO_ACCORD.keys(), key=lambda c: np.linalg.norm(np.array(c) - np.array(color)))\n",
    "        accords.extend(COLOR_TO_ACCORD[closest_color])\n",
    "    return list(set(accords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Recommendation Function\n",
    "This function processes the image, extracts dominant colors, matches them with accords, and filters perfumes based on the given criteria.\n",
    "How filtering works:\n",
    "1. **`preference`** → Filters perfumes that **contain** a specific scent profile in the `Olfactive Profile` column.\n",
    "2. **`exclude`** → Removes perfumes that **contain** a specific scent profile in the `Olfactive Profile` column.\n",
    "3. **`gender`** → Filters perfumes based on the `Gender` column (`Male`, `Female`, `Unisex`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2c187cdcdb40cea9ca49221a9b3010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.png,.jpg,.jpeg', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SITUATION_TO_ACCORD = {\n",
    "    \"formal\": [\"woody\", \"spicy\", \"leather\"],\n",
    "    \"casual\": [\"fresh\", \"citrus\", \"aquatic\"],\n",
    "    \"romantic\": [\"floral\", \"sweet\", \"musky\"],\n",
    "    \"sport\": [\"green\", \"aquatic\", \"ozonic\"],\n",
    "    \"office\": [\"clean\", \"powdery\", \"aldehyde\"]\n",
    "}\n",
    "\n",
    "def recommend_fragrance(image_path, situation=None, preference=None, exclude=None, gender=None, num_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommends perfumes based on extracted colors, situation, and filters.\n",
    "    \"\"\"\n",
    "    segmented_image_path = remove_background(image_path)\n",
    "    dominant_colors = extract_dominant_colors(segmented_image_path)\n",
    "    accords_segmented = map_colors_to_accords(dominant_colors)\n",
    "    \n",
    "    combined_accords = accords_segmented[:]\n",
    "    if situation in SITUATION_TO_ACCORD:\n",
    "        combined_accords.extend(SITUATION_TO_ACCORD[situation])\n",
    "    combined_accords = list(set(combined_accords))\n",
    "    \n",
    "    fragrance_df = pd.read_csv(\"../data/fragrance_ML_model.csv\")\n",
    "    filtered_df = fragrance_df[fragrance_df[\"Olfactive Profile\"].str.contains('|'.join(combined_accords), case=False, na=False)]\n",
    "    \n",
    "    if gender:\n",
    "        filtered_df = filtered_df[filtered_df[\"Gender\"].str.contains(gender, case=False, na=False)]\n",
    "    if preference:\n",
    "        filtered_df = filtered_df[filtered_df[\"Olfactive Profile\"].str.contains(preference, case=False, na=False)]\n",
    "    if exclude:\n",
    "        filtered_df = filtered_df[~filtered_df[\"Olfactive Profile\"].str.contains(exclude, case=False, na=False)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        return \"No perfumes found after applying filters. Try adjusting your criteria.\"\n",
    "    \n",
    "    top_perfumes = filtered_df.sort_values(by=[\"Rating Value\", \"Rating Count\"], ascending=[False, False])\n",
    "    return top_perfumes.head(num_recommendations)\n",
    "\n",
    "uploader = widgets.FileUpload(accept='.png,.jpg,.jpeg', multiple=False)\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uploader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43muploader\u001b[49m\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m      2\u001b[0m     uploaded_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(uploader\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'uploader' is not defined"
     ]
    }
   ],
   "source": [
    "if uploader.value:\n",
    "    uploaded_file = list(uploader.value.values())[0]\n",
    "    image_path = \"test_image.jpg\"\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        f.write(uploaded_file['content'])\n",
    "    print(f\"Image saved as {image_path}\")\n",
    "else:\n",
    "    print(\"No file uploaded! Please upload an image first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "segmented_image_path = remove_background(\"test_image.jpg\")\n",
    "\n",
    "# Show original vs segmented image\n",
    "original = Image.open(\"test_image.jpg\")\n",
    "segmented = Image.open(segmented_image_path)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(segmented)\n",
    "plt.title(\"Segmented Image (No Background)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run a Test Example\n",
    "This runs the function with a sample image, applying filters for situation, preference, exclusion, and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3431706157.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    gender=\"men\",  # Filter by gender\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "recommend_fragrance(\n",
    "    \"image.png\",\n",
    "    situation=\"formal event\",  # Context-based recommendation\n",
    "    preference=\"fresh\",  # Keep only perfumes with \"fresh\"\n",
    "    exclude=\"sweet\",  # Remove perfumes with \"sweet\"\n",
    "    gender=\"men\",  # Filter by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_fragrance(image_path, situation=None, preference=None, exclude=None, gender=None, num_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommends perfumes based on extracted colors from an image, a given situation, \n",
    "    user preferences, exclusions, and gender preference.\n",
    "    \"\"\"\n",
    "    segmented_image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    segmented_pil = Image.fromarray(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB))\n",
    "    segmented_image_path = \"segmented_image.png\"\n",
    "    segmented_pil.save(segmented_image_path)\n",
    "    \n",
    "    dominant_colors = extract_dominant_colors(segmented_image_path)\n",
    "    accords_segmented = map_colors_to_accords(dominant_colors)\n",
    "    \n",
    "    # Combine extracted accords with situation-based accords\n",
    "    combined_accords = accords_segmented[:]\n",
    "    \n",
    "    if situation and situation in SITUATION_TO_ACCORD:\n",
    "        combined_accords.extend(SITUATION_TO_ACCORD[situation])\n",
    "    \n",
    "    combined_accords = list(set(combined_accords))\n",
    "    \n",
    "    # Filter perfumes based on olfactive profile\n",
    "    filtered_df = fragrance_df[fragrance_df[\"Olfactive Profile\"].str.contains('|'.join(combined_accords), case=False, na=False)]\n",
    "    \n",
    "    # Apply gender filter if specified\n",
    "    if gender:\n",
    "        filtered_df = filtered_df[filtered_df[\"Gender\"].str.contains(gender, case=False, na=False)]\n",
    "    \n",
    "    # Apply preference filter if specified\n",
    "    if preference:\n",
    "        filtered_df_pref = filtered_df[filtered_df[\"Olfactive Profile\"].str.contains(preference, case=False, na=False)]\n",
    "        if not filtered_df_pref.empty:\n",
    "            filtered_df = filtered_df_pref  # Use only if results are available\n",
    "    \n",
    "    # Apply exclusion filter if specified\n",
    "    if exclude:\n",
    "        filtered_df_exclude = filtered_df[~filtered_df[\"Olfactive Profile\"].str.contains(exclude, case=False, na=False)]\n",
    "        if not filtered_df_exclude.empty():\n",
    "            filtered_df = filtered_df_exclude  # Use only if results are available\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        return \"No perfumes found after applying filters. Try adjusting your criteria.\"\n",
    "    \n",
    "    # Apply ranking and diversity\n",
    "    top_perfumes = filtered_df.sort_values(by=[\"Rating Value\", \"Rating Count\"], ascending=[False, False])\n",
    "    if len(top_perfumes) > num_recommendations:\n",
    "        diverse_recommendations = random.sample(top_perfumes.head(15).to_dict(\"records\"), num_recommendations)\n",
    "        return pd.DataFrame(diverse_recommendations)\n",
    "    else:\n",
    "        return top_perfumes.head(num_recommendations)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
